"""
FILE 2: EMBEDDINGS
==================

CONCEPT: Convert text to numbers (vectors) that capture semantic meaning

WHAT THIS FILE DOES:
1. Loads a pre-trained embedding model (sentence-transformers)
2. Converts text to 384-dimensional vectors
3. Calculates similarity between vectors (cosine similarity)

WHY WE NEED THIS:
- Computers can't understand words, only numbers
- "Diabetes" and "high blood sugar" are different words but similar meaning
- Embeddings capture that similarity as vector proximity

INTERVIEW Q&A:
--------------
Q: What are embeddings?
A: "Embeddings convert text into dense vectors (lists of numbers) where similar
   meanings result in similar vectors. For example, 'diabetes' and 'high blood sugar'
   will have vectors that are 'close' to each other in 384-dimensional space."

Q: Why use sen tence-transformers instead of Word2Vec or BERT directly?
A: "Sentence-transformers are specifically trained for semantic similarity tasks.
   Unlike BERT (which is trained for classification), sentence-transformers use
   Siamese networks trained on similarity pairs. This makes them better for RAG."

Q: Why 384 dimensions instead of 768 or 1536?
A: "Trade-off analysis:
   - 384-dim (MiniLM): 2x faster, 50% smaller storage, ~2% accuracy drop
   - 768-dim (BERT-base): Baseline
   - 1536-dim (OpenAI): Most accurate but 4x slower, expensive
   
   For real-time chatbot, I chose speed over marginal accuracy gains."

Q: How does cosine similarity work?
A: "Cosine similarity measures the angle between two vectors, not distance.
   Two vectors pointing in same direction (similar meaning) = high similarity (close to 1)
   Two vectors at 90Â° (unrelated) = 0
   This is better than Euclidean distance because it ignores magnitude (text length)."
"""

from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Union
import time


class EmbeddingGenerator:
    """
    Generates semantic embeddings for RAG system.
    
    Design Decision: Use sentence-transformers/all-MiniLM-L6-v2
    WHY:
    - Small: Only 80MB (vs 400MB for MPNet)
    - Fast: 120ms per sentence (vs 200ms for MPNet)
    - Good quality: ~80% of MPNet performance
    - Free: No API calls needed
    
    Interview Note:
    "I chose MiniLM because for an MVP, 80MB and 120ms latency is better
     than 400MB and 200ms for only 2-3% accuracy improvement. In production,
     I'd A/B test to see if users notice the difference."
    """
    
    def __init__(self, model_name: str = 'all-miniLM-L6-v2'):
        """
        Initialize embedding model.
        
        Args:
            model_name: HuggingFace model name (default: all-MiniLM-L6-v2)
                       Full name: sentence-transformers/all-MiniLM-L6-v2
        
        Model Details:
        - Dimensions: 384
        - Max tokens: 512
        - Training: Trained on 1B+ sentence pairs
        - Performance: ~80 on STS benchmark
        
        Interview Note:
        "The model downloads ~80MB on first run. I cache it in ~/.cache/
         so subsequent runs are instant. In production, I'd package the
         model with Docker image to avoid download delays."
        """
        print(f"\nðŸ”„ Loading embedding model: {model_name}")
        print(f"   (Downloads ~80MB on first run, then cached)")
        
        start_time = time.time()
        
        # Load pre-trained model
        # This uses transfer learning - we don't train, just use pre-trained weights
        self.model = SentenceTransformer(model_name)
        
        load_time = time.time() - start_time
        
        print(f"âœ… Model loaded in {load_time:.2f}s")
        print(f"   Embedding dimension: {self.get_embedding_dimension()}")
        print(f"   Max sequence length: {self.model.max_seq_length} tokens")
    
    
    def get_embedding_dimension(self) -> int:
        """
        Get the size of embedding vectors.
        
        Returns:
            384 (for MiniLM)
            
        Interview Note:
        "384 dimensions means each word/sentence is represented by 384 numbers.
         More dimensions = more detailed representation but slower and uses more memory."
        """
        return self.model.get_sentence_embedding_dimension()
    
    
    def embed_text(self, text: Union[str, List[str]]) -> np.ndarray:
        """
        Convert text to embedding vector(s).
        
        Args:
            text: Single string or list of strings
            
        Returns:
            numpy array of shape:
            - (384,) for single string
            - (n, 384) for list of n strings
            
        How it Works (Simplified):
        1. Tokenize: Split text into words/subwords
        2. Embed tokens: Look up each token's vector
        3. Pass through transformer: 12 layers of self-attention
        4. Pool: Average all token vectors â†’ single sentence vector
        5. Normalize: Scale to unit length (for cosine similarity)
        
        Interview Note:
        "The model uses BERT architecture (12 transformer layers).
         Each layer learns different aspects - early layers learn syntax,
         later layers learn semantics. The final pooled vector captures
         the overall meaning of the sentence."
        """
        # Convert single string to list for consistency
        if isinstance(text, str):
            text = [text]
            single_input = True
        else:
            single_input = False
        
        # Generate embeddings
        # normalize_embeddings=True scales vectors to unit length (important for cosine sim)
        embeddings = self.model.encode(
            text,
            normalize_embeddings=True,  # L2 normalization
            show_progress_bar=False
        )
        
        # Return single vector if input was single string
        if single_input:
            return embeddings[0]
        
        return embeddings
    
    
    def batch_embed(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        Embed multiple texts efficiently in batches.
        
        Args:
            texts: List of strings to embed
            batch_size: Process this many texts at once (default: 32)
            
        Returns:
            numpy array of shape (n, 384)
            
        Why Batching:
        - GPU/CPU can process multiple sentences in parallel
        - Batching reduces overhead
        - 32 sentences in batch vs 32 individual calls = 10x faster!
        
        Interview Note:
        "When processing large datasets, batching is critical for performance.
         I chose batch_size=32 as a balance - larger batches use more memory
         but are faster. With 32, I can process 1000 docs in ~30 seconds."
        """
        print(f"\nðŸ”„ Embedding {len(texts)} texts in batches of {batch_size}...")
        
        start_time = time.time()
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            normalize_embeddings=True,
            show_progress_bar=True
        )
        
        duration = time.time() - start_time
        
        print(f"âœ… Embedded {len(texts)} texts in {duration:.2f}s")
        print(f"   Avg: {1000 * duration / len(texts):.0f}ms per text")
        
        return embeddings
    
    
    @staticmethod
    def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
        """
        Calculate cosine similarity between two vectors.
        
        Args:
            vec1: First vector (384-dim)
            vec2: Second vector (384-dim)
            
        Returns:
            Similarity score from -1 to 1:
            - 1.0 = Identical (same direction)
            - 0.0 = Orthogonal (unrelated)
            - -1.0 = Opposite (contradictory)
            
        Formula:
        cosine_sim(A, B) = (A Â· B) / (||A|| Ã— ||B||)
        
        Where:
        - A Â· B = dot product (sum of element-wise multiplication)
        - ||A|| = L2 norm (length of vector)
        
        WHY cosine instead of Euclidean distance:
        - Cosine measures angle (direction), not distance
        - Works better for text where length varies
        - "diabetes" (1 word) vs "symptoms of diabetes" (3 words)
          might have different magnitudes but same meaning (direction)
        
        Interview Note:
        "If vectors are normalized (length=1), then cosine similarity
         simplifies to just dot product: A Â· B. That's why I normalize
         embeddings - makes similarity calculation faster!"
        """
        # Dot product
        dot_product = np.dot(vec1, vec2)
        
        # L2 norms
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        # Cosine similarity
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        similarity = dot_product / (norm1 * norm2)
        
        return float(similarity)
    
    
    @staticmethod
    def batch_cosine_similarity(query_vec: np.ndarray, vectors: np.ndarray) -> np.ndarray:
        """
        Calculate cosine similarity between query and multiple vectors (optimized).
        
        Args:
            query_vec: Single query vector (384,)
            vectors: Multiple vectors (n, 384)
            
        Returns:
            Array of similarity scores (n,)
            
        Optimization:
        Instead of looping (slow):
            for vec in vectors:
                sim = cosine_similarity(query_vec, vec)
        
        Use matrix multiplication (fast):
            similarities = vectors @ query_vec
        
        Why faster:
        - Matrix multiplication is vectorized (uses CPU SIMD or GPU)
        - 100x faster for large datasets!
        
        Interview Note:
        "This is a key optimization for vector search. Instead of comparing
         query to each document sequentially (O(n) comparisons), I use
         matrix multiplication to compare to all documents at once.
         For 10,000 documents, this is 100x faster!"
        """
        # Normalize query vector
        query_vec = query_vec / np.linalg.norm(query_vec)
        
        # Normalize all vectors
        norms = np.linalg.norm(vectors, axis=1, keepdims=True)
        normalized_vectors = vectors / norms
        
        # Batch dot product (matrix multiplication)
        similarities = normalized_vectors @ query_vec
        
        return similarities


# ==============================================================================
# DEMO & TESTING
# ==============================================================================

def demo_embeddings():
    """
    Demo script showing how embeddings work.
    
    Run this to test: python minimal_rag/2_create_embeddings.py
    """
    print("="*70)
    print(" "*22 + "EMBEDDINGS DEMO")
    print("="*70)
    
    # Initialize embedding generator
    generator = EmbeddingGenerator()
    
    # Test sentences
    sentences = [
        "What are the symptoms of diabetes?",
        "How do I know if I have diabetes?",  # Similar to first one!
        "Diabetes warning signs",              # Also similar!
        "What is the best pasta recipe?",      # Completely different!
        "Type 2 diabetes indicators"           # Medical but different focus
    ]
    
    print("\n" + "="*70)
    print("TEST 1: Generate Embeddings")
    print("="*70)
    
    # Generate embeddings
    embeddings = generator.embed_text(sentences)
    
    print(f"\nðŸ“Š Generated {len(embeddings)} embeddings")
    print(f"   Shape: {embeddings.shape}")  # (5, 384)
    
    # Show first embedding
    print(f"\nðŸ” First embedding (first 10 values):")
    print(f"   {embeddings[0][:10]}")
    print(f"   ... (384 values total)")
    
    # Test similarity
    print("\n" + "="*70)
    print("TEST 2: Semantic Similarity")
    print("="*70)
    query = sentences[0]
    query_emb = embeddings[0]
    
    print(f'\nQuery: "{query}"')
    print("\nComparing with other sentences:\n")
    
    for i, sent in enumerate(sentences):
        sim = generator.cosine_similarity(query_emb, embeddings[i])
        
        # Visual bar
        bar_length = int(sim * 40)
        bar = "â–ˆ" * bar_length
        
        # Color indicator
        if sim > 0.8:
            indicator = "ðŸŸ¢ VERY SIMILAR"
        elif sim > 0.5:
            indicator = "ðŸŸ¡ SOMEWHAT SIMILAR"
        else:
            indicator = "ðŸ”´ NOT SIMILAR"
        
        print(f"{indicator} ({sim:.3f}) {bar}")
        print(f'   "{sent}"')
        print()
    
    # Test batch similarity (optimized)
    print("=" *70)
    print("TEST 3: Batch Similarity (Optimized)")
    print("="*70)
    
    similarities = generator.batch_cosine_similarity(query_emb, embeddings)
    print(f"\nâœ… Computed similarities for all {len(embeddings)} sentences at once!")
    print(f"   Results: {similarities}")
    
    # Key insights
    print("\n" + "="*70)
    print("KEY TAKEAWAYS")
    print("="*70)
    print("""
    1. âœ… Each sentence â†’ 384 numbers (384-dimensional vector)
    2. âœ… Similar meanings â†’ similar vectors (high cosine similarity)
    3. âœ… "diabetes symptoms" vs "diabetes warning signs" â†’ 0.90+ similarity
    4. âœ… "diabetes" vs "pasta recipe" â†’ <0.20 similarity
    5. âœ… Batch operations are 100x faster than loops!
    
    In Interview:
    "I use sentence-transformers MiniLM for embeddings. It converts text to
     384-dimensional vectors trained on semantic similarity. When user asks
     'high blood sugar', the embedding is similar to 'diabetes' even though
     words are different. This enables semantic search.
     
     Key optimization: Instead of comparing query to each document in a loop
     (slow), I use matrix multiplication to compare to all documents at once
     (100x faster for 10K documents)."
    """)


if __name__ == "__main__":
    demo_embeddings()
